{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "path = '/mnt/training/twitter/firehose/2018/01/08/18/twitterstream-1-2018-01-08-18-48-00-bcf3d615-9c04-44ec-aac9-25f966490aa4'\n",
    "# instantiate Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the records in the file and saving the result to `dfCount`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfCount = df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a schema for the JSON data to extract just the information that is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, ArrayType, StringType, IntegerType, LongType\n",
    "\n",
    "fullTweetSchema = StructType([\n",
    "  StructField(\"id\", LongType(), True),\n",
    "  StructField(\"user\", StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"screen_name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"friends_count\", IntegerType(), True),\n",
    "    StructField(\"followers_count\", IntegerType(), True),\n",
    "    StructField(\"description\", StringType(), True)\n",
    "  ]), True),\n",
    "  StructField(\"entities\", StructType([\n",
    "    StructField(\"hashtags\", ArrayType(\n",
    "      StructType([\n",
    "        StructField(\"text\", StringType(), True)\n",
    "      ]),\n",
    "    ), True),\n",
    "    StructField(\"urls\", ArrayType(\n",
    "      StructType([\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"expanded_url\", StringType(), True),\n",
    "        StructField(\"display_url\", StringType(), True)\n",
    "      ]),\n",
    "    ), True)\n",
    "  ]), True),\n",
    "  StructField(\"lang\", StringType(), True),\n",
    "  StructField(\"text\", StringType(), True),\n",
    "  StructField(\"created_at\", StringType(), True)\n",
    "])\n",
    "\n",
    "fullTweetDF = spark.read.schema(fullTweetSchema).json(path)\n",
    "fullTweetDF.printSchema()\n",
    "display(fullTweetDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Tables\n",
    "\n",
    "Apply the schema to create tables for relational data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Nulls\n",
    "\n",
    "The Twitter data contains both deletions and tweets.  This is why some records appear as null values. Creating a DataFramed called `fullTweetFilteredDF` that filters out the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "fullTweetFilteredDF = (fullTweetDF\n",
    "  .filter(col(\"id\").isNotNull())\n",
    ")\n",
    "\n",
    "display(fullTweetFilteredDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter uses a non-standard timestamp format that Spark doesn't recognize. Currently the `created_at` column is formatted as a string. Parsing the timestamp column using `unix_timestamp`, and cast the result as `TimestampType`. The timestamp format is `EEE MMM dd HH:mm:ss ZZZZZ yyyy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "timestampFormat = \"EEE MMM dd HH:mm:ss ZZZZZ yyyy\"\n",
    "\n",
    "tweetDF = fullTweetFilteredDF.select(col(\"id\").alias(\"tweetID\"), \n",
    "  col(\"user.id\").alias(\"userID\"), \n",
    "  col(\"lang\").alias(\"language\"),\n",
    "  col(\"text\"),\n",
    "  unix_timestamp(\"created_at\", timestampFormat).cast(TimestampType()).alias(\"createdAt\")\n",
    ")\n",
    "\n",
    "display(tweetDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accountDF = fullTweetFilteredDF.select(col(\"user.id\").alias(\"userID\"), \n",
    "    col(\"user.screen_name\").alias(\"screenName\"),\n",
    "    col(\"user.location\"),\n",
    "    col(\"user.friends_count\").alias(\"friendsCount\"),\n",
    "    col(\"user.followers_count\").alias(\"followersCount\"),\n",
    "    col(\"user.description\")\n",
    ")\n",
    "\n",
    "display(accountDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "hashtagDF = fullTweetFilteredDF.select(col(\"id\").alias(\"tweetID\"), \n",
    "    explode(col(\"entities.hashtags.text\")).alias(\"hashtag\")\n",
    ")\n",
    "\n",
    "hashtagDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urlDF = (fullTweetFilteredDF.select(col(\"id\").alias(\"tweetID\"), \n",
    "    explode(col(\"entities.urls\")).alias(\"urls\"))\n",
    "  .select(\n",
    "    col(\"tweetID\"),\n",
    "    col(\"urls.url\").alias(\"URL\"),\n",
    "    col(\"urls.display_url\").alias(\"displayURL\"),\n",
    "    col(\"urls.expanded_url\").alias(\"expandedURL\"))\n",
    ")\n",
    "\n",
    "urlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading the Results\n",
    "\n",
    "Saving the DataFrames in Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accountDF.write.mode(\"overwrite\").parquet(\"/tmp/account.parquet\")\n",
    "tweetDF.write.mode(\"overwrite\").parquet(\"/tmp/tweet.parquet\")\n",
    "hashtagDF.write.mode(\"overwrite\").parquet(\"/tmp/hashtag.parquet\")\n",
    "urlDF.write.mode(\"overwrite\").parquet(\"/tmp/url.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "08-Capstone-Project",
  "notebookId": 183725293955610
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
